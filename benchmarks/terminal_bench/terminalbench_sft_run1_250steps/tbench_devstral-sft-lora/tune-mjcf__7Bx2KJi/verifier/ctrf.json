{
    "results": {
        "tool": {
            "name": "pytest",
            "version": "8.4.1"
        },
        "summary": {
            "tests": 4,
            "passed": 3,
            "failed": 1,
            "skipped": 0,
            "pending": 0,
            "other": 0,
            "start": 1768180336.359224,
            "stop": 1768180363.7687705
        },
        "tests": [
            {
                "name": "test_outputs.py::test_model_ref_unchanged",
                "status": "passed",
                "duration": 0.0006374290096573532,
                "start": 1768180337.2768323,
                "stop": 1768180337.2777455,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_tuned_model_exists",
                "status": "passed",
                "duration": 0.00040323997382074594,
                "start": 1768180337.2779915,
                "stop": 1768180337.278541,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_correctness",
                "status": "passed",
                "duration": 1.9151145500363782,
                "start": 1768180337.278723,
                "stop": 1768180339.1940987,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_model_speed",
                "status": "failed",
                "raw_status": "call_failed",
                "duration": 24.55013503198279,
                "start": 1768180339.194368,
                "stop": 1768180363.7684398,
                "retries": 0,
                "file_path": "test_outputs.py",
                "trace": "def test_model_speed():\n        \"\"\"Test that new model is faster than the reference model\"\"\"\n        model_path = app_dir / \"model.xml\"\n        model_ref_path = app_dir / \"model_ref.xml\"\n    \n        model = mujoco.MjModel.from_xml_path(str(model_path))\n        model_ref = mujoco.MjModel.from_xml_path(str(model_ref_path))\n    \n        times_model = simulation_time(model, n_runs=n_runs)\n        times_model = drop_extreme_percentiles(times_model, 5, 95)\n        times_model_ref = simulation_time(model_ref, n_runs=n_runs)\n        times_model_ref = drop_extreme_percentiles(times_model_ref, 5, 95)\n        speedup = (times_model_ref / times_model).mean().item()\n        act_time_pctg = (times_model / times_model_ref).mean().item()\n    \n        print(f\"Avg simulation time: {times_model.mean().item():.4f} secs\")\n        print(f\"Avg simulation time (ref): {times_model_ref.mean().item():.4f} secs\")\n        print(f\"Speedup: {speedup:.2f}x\")\n        print(f\"Time pctg: {act_time_pctg:.2f}\")\n    \n>       assert act_time_pctg <= pctg, (\n            f\"Time pctg {act_time_pctg * 100:.2f}% (need {pctg * 100:.2f}%)\"\n        )\nE       AssertionError: Time pctg 96.06% (need 60.00%)\nE       assert 0.9605517747218938 <= 0.6\n\n/tests/test_outputs.py:111: AssertionError",
                "message": "The test failed in the call phase due to an assertion error"
            }
        ]
    }
}