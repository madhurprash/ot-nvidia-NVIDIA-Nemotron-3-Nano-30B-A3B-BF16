{
    "results": {
        "tool": {
            "name": "pytest",
            "version": "8.4.1"
        },
        "summary": {
            "tests": 6,
            "passed": 5,
            "failed": 1,
            "skipped": 0,
            "pending": 0,
            "other": 0,
            "start": 1768172461.4703677,
            "stop": 1768172461.568959
        },
        "tests": [
            {
                "name": "test_outputs.py::test_output_files_exist",
                "status": "passed",
                "duration": 0.0005967780598439276,
                "start": 1768172461.508207,
                "stop": 1768172461.509046,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_input_data_integrity",
                "status": "passed",
                "duration": 0.0007408399833366275,
                "start": 1768172461.5092578,
                "stop": 1768172461.510148,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_generate_and_schema",
                "status": "passed",
                "duration": 0.010813036002218723,
                "start": 1768172461.510325,
                "stop": 1768172461.5212927,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_solution_shape_feasibility_and_batch_consistency",
                "status": "passed",
                "duration": 0.010058898013085127,
                "start": 1768172461.5214736,
                "stop": 1768172461.5316768,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_solution_coverage_no_duplicates",
                "status": "passed",
                "duration": 0.008494375040754676,
                "start": 1768172461.5318758,
                "stop": 1768172461.5405173,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_performance_thresholds",
                "status": "failed",
                "raw_status": "call_failed",
                "duration": 0.012685476976912469,
                "start": 1768172461.5406952,
                "stop": 1768172461.5687726,
                "retries": 0,
                "file_path": "test_outputs.py",
                "trace": "def test_performance_thresholds() -> None:\n        \"\"\"Test solution meets performance thresholds described in task.yaml.\n    \n        **Performance Thresholds:**\n        - Bucket 1: cost \u2264 3.0e11, pad_ratio \u2264 0.055, p95_latency \u2264 2.1e6 ms,\n          sequential_timecost \u2264 2.7e8 ms\n        - Bucket 2: cost \u2264 4.8e10, pad_ratio \u2264 0.15, p95_latency \u2264 2.1e5 ms,\n          sequential_timecost \u2264 3.2e7 ms\n    \n        **Optimization Verification:**\n        - Global shape constraint: \u2264 8 unique shapes across all buckets\n    \n        \"\"\"\n    \n        req1_path = \"/app/task_file/input_data/requests_bucket_1.jsonl\"\n        plan1_path = \"/app/task_file/output_data/plan_b1.jsonl\"\n        m_b1 = _plan_metrics(req1_path, plan1_path)\n    \n        req2_path = \"/app/task_file/input_data/requests_bucket_2.jsonl\"\n        plan2_path = \"/app/task_file/output_data/plan_b2.jsonl\"\n        m_b2 = _plan_metrics(req2_path, plan2_path)\n    \n        # Validate against tight absolute performance thresholds\n        # Bucket 1\n>       assert m_b1[\"cost\"] <= 3.0e11, f\"bucket1 cost {m_b1['cost']} > 3.0e11\"\nE       AssertionError: bucket1 cost 2448911082464.0 > 3.0e11\nE       assert 2448911082464.0 <= 300000000000.0\n\n/tests/test_outputs.py:322: AssertionError",
                "message": "The test failed in the call phase due to an assertion error"
            }
        ]
    }
}