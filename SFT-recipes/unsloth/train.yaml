# Training configuration for NVIDIA Nemotron-3-Nano-30B-A3B fine-tuning using Unsloth

# Model configuration
model:
  name: "unsloth/Nemotron-3-Nano-30B-A3B"
  max_seq_length: 2048  # Reduced to 1024 for memory efficiency
  load_in_4bit: True  # Enable 4-bit quantization to reduce memory usage
  load_in_8bit: False
  full_finetuning: False  # Disable full fine-tuning, use LoRA instead
  trust_remote_code: True
  attn_implementation: "eager"
  unsloth_force_compile: True

# LoRA configuration
lora:
  # Enable LoRA for memory-efficient fine-tuning
  enabled: true  # ENABLED for parameter-efficient training
  r: 8  # Increased rank for better capacity
  lora_alpha: 16  # 2x rank for scaling
  lora_dropout: 0  # Small dropout for regularization
  bias: "none"
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
    - "in_proj"
    - "out_proj"
  use_gradient_checkpointing: "unsloth"
  use_rslora: False
  random_state: 3407
  loftq_config: None

# Dataset configuration
dataset:
  name: "open-thoughts/OpenThoughts-Agent-v1-SFT"
  config: "default"
  split: "train"
  num_train_samples: null  # Set to a number to limit training samples for debugging

# Training arguments
training:
  output_dir: "./outputs/nemotron-sft"
  per_device_train_batch_size: 1  # Reduced to 1 for memory safety
  gradient_accumulation_steps: 16  # Increased to maintain effective batch size = 16
  warmup_steps: 5
  max_steps: 60
  num_train_epochs: 1  # Will be ignored if max_steps is set
  learning_rate: 2.0e-4
  logging_steps: 1
  optim: "adamw_8bit"  # 8-bit optimizer for memory efficiency
  weight_decay: 0.001
  lr_scheduler_type: "linear"
  seed: 3407
  report_to: "none"
  save_steps: 500
  save_total_limit: 2
  gradient_checkpointing: true  # Enable gradient checkpointing for memory efficiency
  bf16: true  # Enable BF16 training to match model dtype
  # Multi-GPU DDP settings
  ddp_find_unused_parameters: True  # Set to false for better performance with LoRA
  ddp_backend: "nccl"  # Use NCCL backend for GPU communication
  
# Output configuration
output:
  save_merged_model: true  # Save LoRA-merged model for inference
  merged_output_dir: "./outputs/nemotron-sft-merged"
